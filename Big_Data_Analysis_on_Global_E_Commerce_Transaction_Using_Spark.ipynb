{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPub+scAkobdDf6PhIjzhQg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sunnykumar1554/Ad-s-/blob/main/Big_Data_Analysis_on_Global_E_Commerce_Transaction_Using_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task-4 Big Data Analytic using Apache Spark"
      ],
      "metadata": {
        "id": "KDWJMvGLak2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1 :- Load Dataset"
      ],
      "metadata": {
        "id": "i6SdiWaYa-rA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark openpyxl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7WeeV00SHGt",
        "outputId": "cc8df334-e57b-4998-851f-4694663d8282"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import FloatType, DateType\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"OnlineRetailII_Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "PrD1FAQWSLqY",
        "outputId": "54812556-cb4b-4e07-b865-2618f5d97ed9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c71e9566a20>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0d0a1c9572e1:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v4.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>OnlineRetailII_Analysis</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the data to Google Colab"
      ],
      "metadata": {
        "id": "7fuoiFQTbMBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # choose online_retail_II.xlsx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "TKqu_RKsVki7",
        "outputId": "f6ed3ab4-563f-458a-d599-624aa104f68f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f1379934-e93b-4c86-b594-7b437987fe91\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f1379934-e93b-4c86-b594-7b437987fe91\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving online_retail_II.xlsx to online_retail_II.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wEMjwmiWi73C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the data using Spark"
      ],
      "metadata": {
        "id": "OndyO1FBbT6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "excel_path = \"/content/online_retail_II.xlsx\"\n",
        "\n",
        "# Read first sheet (or specify sheet_name if needed)\n",
        "pdf_raw = pd.read_excel(excel_path)\n",
        "\n",
        "pdf_raw.head()\n"
      ],
      "metadata": {
        "id": "hdgNrWPiSTw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display first 10 rows,\n",
        "print schema,\n",
        "count total records"
      ],
      "metadata": {
        "id": "eDA_j188bcba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. How many rows does the dataset countain?\n"
      ],
      "metadata": {
        "id": "zojDFCB5dccW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 525461"
      ],
      "metadata": {
        "id": "jQPk4yCtdp-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What issues do you observe in the data?"
      ],
      "metadata": {
        "id": "1AtTTvtwd0GT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qZkNQON2d94E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = spark.createDataFrame(pdf_raw)\n",
        "df_raw.printSchema()\n",
        "df_raw.show(10, truncate=False)\n",
        "\n",
        "row_count = df_raw.count()\n",
        "print(\"Total rows in raw dataset:\", row_count)\n"
      ],
      "metadata": {
        "id": "1fzdyVeFYgDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: Data Cleaning using Spark(core Big Data skill)"
      ],
      "metadata": {
        "id": "gt48e4-9efDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 Remove roe where:\n",
        "\n",
        "*   CostomerID is null\n",
        "*   Description is null\n",
        "\n"
      ],
      "metadata": {
        "id": "hJSxV7Czcr1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df_raw.dropna(subset=[\"Customer ID\", \"Description\"])\n"
      ],
      "metadata": {
        "id": "NDGaKfxGYutl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 Convert:\n",
        "\n",
        "*   InvoiceData to Date type\n",
        "*   UnitPrice to Float\n",
        "\n"
      ],
      "metadata": {
        "id": "7x6_QWzueqG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If InvoiceDate is a string with date + time, we convert it to DateType\n",
        "df_clean = df_clean.withColumn(\n",
        "    \"InvoiceDate\",\n",
        "    F.to_date(\"InvoiceDate\")  # if fails, we will parse format explicitly\n",
        ")\n"
      ],
      "metadata": {
        "id": "Qj-_mukqYyA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative robust version:\n",
        "df_clean = df_clean.withColumn(\n",
        "    \"InvoiceDate_str\",\n",
        "    F.col(\"InvoiceDate\").cast(\"string\")\n",
        ")\n",
        "\n",
        "df_clean = df_clean.withColumn(\n",
        "    \"InvoiceDate\",\n",
        "    F.to_date(\"InvoiceDate_str\", \"yyyy-MM-dd\")  # change if your format is different\n",
        ").drop(\"InvoiceDate_str\")\n"
      ],
      "metadata": {
        "id": "U3KrWuHDYzp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df_clean.withColumn(\n",
        "    \"Price\",\n",
        "    F.col(\"Price\").cast(FloatType())\n",
        ")\n"
      ],
      "metadata": {
        "id": "Jv0wDd2LY3tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 Remove negative Quantity values"
      ],
      "metadata": {
        "id": "4BDAclYKiRTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df_clean.filter(F.col(\"Quantity\") > 0)\n"
      ],
      "metadata": {
        "id": "4ZU0Ha2KY6q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 Create new column:"
      ],
      "metadata": {
        "id": "FWORizgki81r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df_clean.withColumn(\n",
        "    \"TotalAmount\",\n",
        "    F.col(\"Quantity\") * F.col(\"Price\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "RiHbrXPFY-9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 TotalAmount = Quantity * UnitPrice"
      ],
      "metadata": {
        "id": "xzNJIGiTjFUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show cleaned DataFrame"
      ],
      "metadata": {
        "id": "Y94r9GjCjQ14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print updated record count = 407695"
      ],
      "metadata": {
        "id": "yQe1hZG-jW1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.show(10, truncate=False)\n",
        "\n",
        "clean_count = df_clean.count()\n",
        "print(\"Rows after cleaning:\", clean_count)\n"
      ],
      "metadata": {
        "id": "8B8hsi7VZBZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3: Business Analysis Tasks"
      ],
      "metadata": {
        "id": "GtNEABuwjwFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part (A) - Top 10 Selling Products\n",
        "\n",
        "*   Product descriptions with highest total sales value\n",
        "\n"
      ],
      "metadata": {
        "id": "NHgztXrUj3ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_products = df_clean.groupBy(\"Description\") \\\n",
        "    .agg(F.sum(\"TotalAmount\").alias(\"total_sales\")) \\\n",
        "    .orderBy(F.desc(\"total_sales\")) \\\n",
        "    .limit(10)\n",
        "\n",
        "top_products.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "0bYO7RxsZNEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task (B) - Country-wise Revenue\n",
        "\n",
        "*   Total revenue per country\n",
        "*   Sort in descending order\n",
        "\n"
      ],
      "metadata": {
        "id": "BkgJ2HXfkGqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "country_revenue = df_clean.groupBy(\"Country\") \\\n",
        "    .agg(F.sum(\"TotalAmount\").alias(\"total_revenue\")) \\\n",
        "    .orderBy(F.desc(\"total_revenue\"))\n",
        "\n",
        "country_revenue.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "u9TvFfNZZPat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task (C) – Customer Purchase Behaviour\n",
        "\n",
        "*   Top 10 customers by total spending\n",
        "\n"
      ],
      "metadata": {
        "id": "cuINONxKkhvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_customers = df_clean.groupBy(\"Customer ID\") \\\n",
        "    .agg(F.sum(\"TotalAmount\").alias(\"total_spent\")) \\\n",
        "    .orderBy(F.desc(\"total_spent\")) \\\n",
        "    .limit(10)\n",
        "\n",
        "top_customers.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "-X8j0eEQZTC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task (D) – Monthly Sales Trend\n",
        "\n",
        "*   Extract month from InvoiceDate and calculate total monthly revenue.\n",
        "*   Students must display results in tabular form.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1F-5o8cVksn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_monthly = df_clean.withColumn(\"Year\", F.year(\"InvoiceDate\")) \\\n",
        "                     .withColumn(\"Month\", F.month(\"InvoiceDate\"))\n",
        "\n",
        "monthly_sales = df_monthly.groupBy(\"Year\", \"Month\") \\\n",
        "    .agg(F.sum(\"TotalAmount\").alias(\"monthly_revenue\")) \\\n",
        "    .orderBy(\"Year\", \"Month\")\n",
        "\n",
        "monthly_sales.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "Vgt1LFdTZUnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_pdf = monthly_sales.toPandas()\n",
        "monthly_pdf.head()\n"
      ],
      "metadata": {
        "id": "2inWKai3ZaUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 4: Big Data Performance Comparison"
      ],
      "metadata": {
        "id": "K-OiyGXPlbyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal Approach (Pandas)"
      ],
      "metadata": {
        "id": "qYk791y-lid_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Read Excel again with Pandas\n",
        "pdf = pd.read_excel(excel_path)\n",
        "\n",
        "# Basic cleaning similar to Spark\n",
        "pdf = pdf.dropna(subset=[\"Customer ID\", \"Description\"])\n",
        "pdf = pdf[pdf[\"Quantity\"] > 0]\n",
        "\n",
        "pdf[\"TotalAmount\"] = pdf[\"Quantity\"] * pdf[\"Price\"]\n",
        "\n",
        "country_rev_pd = pdf.groupby(\"Country\")[\"TotalAmount\"].sum().sort_values(ascending=False)\n",
        "\n",
        "end = time.time()\n",
        "pandas_time = end - start\n",
        "\n",
        "print(\"Pandas aggregation time (seconds):\", pandas_time)\n",
        "country_rev_pd.head()\n"
      ],
      "metadata": {
        "id": "3FAXXL9bZfV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Approach"
      ],
      "metadata": {
        "id": "4HpCKjjrlm8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "country_rev_spark = df_clean.groupBy(\"Country\") \\\n",
        "    .agg(F.sum(\"TotalAmount\").alias(\"total_revenue\")) \\\n",
        "    .orderBy(F.desc(\"total_revenue\"))\n",
        "\n",
        "country_rev_spark.show(10, truncate=False)\n",
        "\n",
        "end = time.time()\n",
        "spark_time = end - start\n",
        "\n",
        "print(\"Spark aggregation time (seconds):\", spark_time)\n"
      ],
      "metadata": {
        "id": "X4SPR_UZZu8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which was faster and why?"
      ],
      "metadata": {
        "id": "6byXOvYmls7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In my case Spark is the fastest, it happens because of manily 2 reasons\n",
        "\n",
        "*   Because i am runnig this program in Google colab so the internet connection is one of the main point.\n",
        "*   In the implementation we are not using distributed file system and parallel processing which are the core mechanism Big Data (spark) due to this Pandas may result less processing time at few cases.  \n",
        "\n"
      ],
      "metadata": {
        "id": "2dktRYXUl1aZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What would happen if data grows to 10 million rows?"
      ],
      "metadata": {
        "id": "LpNEl5gul13z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the data grows to 10 million row, In that case Spark will run faster every time.\n",
        "\n",
        "Because spark uses Lazy Evaluti\n",
        "When you perform transformations like:\n",
        "\n",
        "filter, withColumn, dropna, groupBy\n",
        "\n",
        "Spark does not execute them immediately.\n",
        "\n",
        "It waits until you run an action, such as:\n",
        "\n",
        "show()\n",
        "\n",
        "count()\n",
        "\n",
        "collect()\n",
        "\n"
      ],
      "metadata": {
        "id": "Qf_aDER8tSld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 5: Optimization Challenge (Advanced Thinking)"
      ],
      "metadata": {
        "id": "2rZ2kmG_mIvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Students must apply:"
      ],
      "metadata": {
        "id": "8VQ3SYqUmc4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   .cache() to cleaned dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zKlAJjFPmh-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "country_rev_no_cache = df_clean.groupBy(\"Country\") \\\n",
        "    .agg(F.sum(\"TotalAmount\").alias(\"total_revenue\")) \\\n",
        "    .orderBy(F.desc(\"total_revenue\"))\n",
        "\n",
        "country_rev_no_cache.show(10, truncate=False)\n",
        "\n",
        "end = time.time()\n",
        "no_cache_time = end - start\n",
        "\n",
        "print(\"Time without cache:\", no_cache_time)\n"
      ],
      "metadata": {
        "id": "xxn7gpr0Zyat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Re-run one aggregation\n",
        "\n"
      ],
      "metadata": {
        "id": "2VyQ29e0moTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache\n",
        "df_clean_cached = df_clean.cache()\n",
        "\n",
        "# Trigger cache load\n",
        "df_clean_cached.count()\n",
        "\n",
        "# Aggregation with cache\n",
        "start = time.time()\n",
        "\n",
        "country_rev_cache = df_clean_cached.groupBy(\"Country\") \\\n",
        "    .agg(F.sum(\"TotalAmount\").alias(\"total_revenue\")) \\\n",
        "    .orderBy(F.desc(\"total_revenue\"))\n",
        "\n",
        "country_rev_cache.show(10, truncate=False)\n",
        "\n",
        "end = time.time()\n",
        "cache_time = end - start\n",
        "\n",
        "print(\"Time with cache:\", cache_time)\n"
      ],
      "metadata": {
        "id": "_AigBM6YZ226"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Compare time before and after caching\n",
        "\n"
      ],
      "metadata": {
        "id": "UYeoliL2mtce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time without cache aggregation = 3.14...\n",
        "Time with cache aggregation = 0.723...\n",
        "\n"
      ],
      "metadata": {
        "id": "Ts8pTXslmzzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why caching reduced exeution time?\n"
      ],
      "metadata": {
        "id": "IX0m6Oqdn7AN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caching reduced exeution time because spark uses lazy evalution, when you perform transformation like filter,withcolum,dropna,groupby.\n"
      ],
      "metadata": {
        "id": "7pRbnHB7oIQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "spark does not execute them immeditely."
      ],
      "metadata": {
        "id": "ZSSetQzkpl5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It waits until you run an action, such as:\n",
        "\n",
        "*   show()\n",
        "*   count()\n",
        "*   collent()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OOUa2MrCpsPy"
      }
    }
  ]
}